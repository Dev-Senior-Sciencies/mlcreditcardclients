"""
Neural Network Model Module

Este m√≥dulo cont√©m a implementa√ß√£o da rede neural para predi√ß√£o
de inadimpl√™ncia de cart√£o de cr√©dito.

Autor: Samuel Lucas Gon√ßalves Santana
"""

import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score, precision_recall_curve


def cross_validate_lambda(X, y, lambdas, params):
    """Seleciona melhor lambda via Cross-Validation.
    
    Testa diferentes valores de regulariza√ß√£o L2 e escolhe
    o que maximiza ROC-AUC na valida√ß√£o cruzada.
    
    Por que Cross-Validation?
    - Evita overfitting
    - Garante que o modelo generalize bem
    - Fornece estimativa robusta da performance
    
    Args:
        X: Features de treino
        y: Target de treino
        lambdas: Lista de valores lambda para testar
        params: Par√¢metros de configura√ß√£o
    
    Returns:
        best_lambda: Melhor valor de regulariza√ß√£o
    """
    skf = StratifiedKFold(
        n_splits=params["cv_folds"],
        shuffle=True,
        random_state=params["random_state"]
    )

    best_lambda = None
    best_auc = -np.inf

    for lambda_ in lambdas:
        aucs = []

        for train_idx, val_idx in skf.split(X, y):

            X_tr, X_val = X[train_idx], X[val_idx]
            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]

            # Arquitetura da Rede Neural
            # 64 ‚Üí 32 ‚Üí 1: Redu√ß√£o progressiva captura padr√µes complexos
            model = tf.keras.Sequential([
                tf.keras.layers.Dense(64, activation='relu',
                      kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
                tf.keras.layers.Dense(32, activation='relu',
                      kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
                tf.keras.layers.Dense(1, activation='linear')  # Logits
            ])

            model.compile(
                optimizer=tf.keras.optimizers.Adam(
                    learning_rate=params["learning_rate"]
                ),
                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)
            )

            model.fit(
                X_tr,
                y_tr,
                epochs=params["epochs_cross"],
                verbose=params["verbose"]
            )

            # Predi√ß√£o e c√°lculo de AUC
            logits = model(X_val)
            y_pred_proba = tf.nn.sigmoid(logits).numpy().ravel()

            auc = roc_auc_score(y_val, y_pred_proba)
            aucs.append(auc)

        mean_auc = np.mean(aucs)
        std_auc = np.std(aucs)

        print(f"[CV] Œª={lambda_:.5f} | AUC={mean_auc:.4f} ¬± {std_auc:.4f}")

        if mean_auc > best_auc:
            best_auc = mean_auc
            best_lambda = lambda_

    return best_lambda


def train_final_model(X_train, X_test, y_train, y_test, best_lambda, params):
    """Treina modelo final com melhor lambda encontrado.
    
    Arquitetura da Rede Neural:
    - Camada 1: 64 neur√¥nios + ReLU + L2 regularization
    - Camada 2: 32 neur√¥nios + ReLU + L2 regularization  
    - Output: 1 neur√¥nio + linear (logits)
    
    Por que essa arquitetura?
    - 64 ‚Üí 32: Redu√ß√£o progressiva captura padr√µes complexos
    - ReLU: Ativa√ß√£o n√£o-linear eficiente
    - Linear output: Para usar BinaryCrossentropy(from_logits=True)
    
    Args:
        X_train, X_test: Features de treino e teste
        y_train, y_test: Targets de treino e teste
        best_lambda: Melhor regulariza√ß√£o encontrada no CV
        params: Par√¢metros de configura√ß√£o
    
    Returns:
        y_proba: Probabilidades preditas no test set
        auc: ROC-AUC no test set
    """
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu',
              kernel_regularizer=tf.keras.regularizers.l2(best_lambda)),
        tf.keras.layers.Dense(32, activation='relu',
              kernel_regularizer=tf.keras.regularizers.l2(best_lambda)),
        tf.keras.layers.Dense(1, activation='linear')
    ])

    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            learning_rate=params["learning_rate"]
        ),
        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)
    )

    model.fit(
        X_train,
        y_train,
        epochs=params["epochs"],
        verbose=params["verbose"]
    )

    # Predi√ß√£o final
    logits = model(X_test)
    y_proba = tf.nn.sigmoid(logits).numpy().ravel()

    auc = roc_auc_score(y_test, y_proba)
    print(f"üìä Neural Network ROC-AUC: {auc:.4f}")

    return y_proba, auc


def find_best_threshold(y_true, y_proba):
    """Encontra threshold √≥timo maximizando F1-score.
    
    Por que otimizar threshold?
    - O threshold padr√£o (0.5) n√£o √© √≥timo para problemas de neg√≥cio
    - F1-score balanceia Precision e Recall
    - Adequado para classes desbalanceadas
    - Foca na classe minorit√°ria (inadimplentes)
    
    Impacto no Neg√≥cio:
    - Threshold menor ‚Üí Mais conservador ‚Üí Menos FN (inadimplentes aprovados)
    
    Args:
        y_true: Labels verdadeiros
        y_proba: Probabilidades preditas
    
    Returns:
        best_threshold: Threshold que maximiza F1-score
    """
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)

    # Calcula F1-score para cada threshold
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)
    best_idx = np.argmax(f1_scores)

    best_threshold = thresholds[best_idx]
    best_f1 = f1_scores[best_idx]

    print(f"üéØ Melhor threshold: {best_threshold:.3f}")
    print(f"üìà Melhor F1-score: {best_f1:.4f}")

    return best_threshold