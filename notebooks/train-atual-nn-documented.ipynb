{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Credit Card Default Prediction - Neural Network\n",
    "\n",
    "Este notebook implementa um modelo de **rede neural** para prever inadimpl√™ncia de clientes de cart√£o de cr√©dito.\n",
    "\n",
    "## üéØ Objetivo\n",
    "Construir um classificador bin√°rio que identifique clientes com alta probabilidade de inadimpl√™ncia no pr√≥ximo m√™s.\n",
    "\n",
    "## üìã Pipeline do Projeto\n",
    "1. **Importa√ß√£o de bibliotecas e configura√ß√µes**\n",
    "2. **Feature Engineering** - Cria√ß√£o de vari√°veis derivadas\n",
    "3. **Pr√©-processamento** - Normaliza√ß√£o e encoding\n",
    "4. **Cross-Validation** - Sele√ß√£o de hiperpar√¢metros\n",
    "5. **Treinamento do modelo final**\n",
    "6. **Otimiza√ß√£o de threshold**\n",
    "7. **Avalia√ß√£o e m√©tricas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Importa√ß√£o de Bibliotecas\n",
    "\n",
    "### Por que essas bibliotecas?\n",
    "- **TensorFlow/Keras**: Framework para redes neurais\n",
    "- **Scikit-learn**: Pr√©-processamento e m√©tricas\n",
    "- **OmegaConf**: Gerenciamento de configura√ß√µes\n",
    "- **Matplotlib**: Visualiza√ß√µes\n",
    "- **XGBoost**: Compara√ß√£o com outros modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "Dense = tf.keras.layers.Dense\n",
    "Input = tf.keras.layers.Input\n",
    "Sequential = tf.keras.Sequential\n",
    "MeanSquaredError = tf.keras.losses.MeanSquaredError\n",
    "BinaryCrossentropy = tf.keras.losses.BinaryCrossentropy\n",
    "Sigmoid = tf.keras.activations.sigmoid\n",
    "\n",
    "file_path = os.getcwd()\n",
    "\n",
    "conf = OmegaConf.load(os.path.join(file_path, \"..\", \"src\", \"config.yml\"))\n",
    "\n",
    "data_path = os.path.join(file_path, \"..\", \"data\", \"UCI_Credit_Card.csv\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "lambdas = [0, 1e-5, 1e-4, 5e-4, 1e-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Feature Engineering\n",
    "\n",
    "### Por que criar novas features?\n",
    "As features originais n√£o capturam completamente o **comportamento financeiro** dos clientes. Criamos vari√°veis derivadas que representam:\n",
    "\n",
    "1. **Capacidade de pagamento** (percentual pago)\n",
    "2. **Utiliza√ß√£o de cr√©dito** (quanto usa do limite)\n",
    "3. **Padr√£o de atraso** (hist√≥rico de inadimpl√™ncia)\n",
    "\n",
    "### üìä Percentual Pago da Fatura\n",
    "Mede quanto o cliente consegue pagar da fatura em rela√ß√£o ao valor devido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_percent_paid_features(df):\n",
    "    \"\"\"Cria features de percentual pago da fatura.\n",
    "    \n",
    "    Para cada m√™s, calcula: PAY_AMT / BILL_AMT\n",
    "    Indica a capacidade de pagamento do cliente.\n",
    "    \"\"\"\n",
    "    for i in range(1, 7):\n",
    "        df[f\"PCT_PAID_{i}\"] = np.where(\n",
    "            df[f\"BILL_AMT{i}\"] > 0,\n",
    "            df[f\"PAY_AMT{i}\"] / df[f\"BILL_AMT{i}\"],\n",
    "            0\n",
    "        )\n",
    "\n",
    "    df[\"PCT_PAID_MEAN\"] = df[[f\"PCT_PAID_{i}\" for i in range(1, 7)]].mean(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_credit_utilization(df):\n",
    "    \"\"\"Cria feature de utiliza√ß√£o de cr√©dito.\n",
    "    \n",
    "    Calcula: m√©dia das faturas / limite de cr√©dito\n",
    "    Indica o quanto o cliente usa do limite dispon√≠vel.\n",
    "    \"\"\"\n",
    "    bill_cols = [f\"BILL_AMT{i}\" for i in range(1, 7)]\n",
    "    df[\"BILL_MEAN\"] = df[bill_cols].mean(axis=1)\n",
    "\n",
    "    df[\"CREDIT_UTILIZATION\"] = np.where(\n",
    "        df[\"LIMIT_BAL\"] > 0,\n",
    "        df[\"BILL_MEAN\"] / df[\"LIMIT_BAL\"],\n",
    "        0\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_pay_delay_features(df):\n",
    "    \"\"\"Cria features de hist√≥rico de atraso.\n",
    "    \n",
    "    Calcula m√©dia e m√°ximo dos atrasos.\n",
    "    Indica o padr√£o de inadimpl√™ncia do cliente.\n",
    "    \"\"\"\n",
    "    pay_cols = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
    "    df[\"PAY_DELAY_MEAN\"] = df[pay_cols].mean(axis=1)\n",
    "    df[\"PAY_DELAY_MAX\"] = df[pay_cols].max(axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"Pipeline completo de feature engineering.\"\"\"\n",
    "    df = df.copy()\n",
    "    df = create_percent_paid_features(df)\n",
    "    df = create_credit_utilization(df)\n",
    "    df = create_pay_delay_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualiza√ß√£o e An√°lise de Neg√≥cio\n",
    "\n",
    "### Por que matriz de confus√£o?\n",
    "Em problemas de cr√©dito, **diferentes tipos de erro t√™m custos diferentes**:\n",
    "- **False Negative (FN)**: Aprovar inadimplente = PERDA FINANCEIRA\n",
    "- **False Positive (FP)**: Recusar bom cliente = PERDA DE OPORTUNIDADE\n",
    "\n",
    "A matriz nos ajuda a **quantificar o impacto de neg√≥cio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_proba, threshold):\n",
    "    \"\"\"Plota matriz de confus√£o e calcula impacto de neg√≥cio.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Labels verdadeiros\n",
    "        y_proba: Probabilidades preditas\n",
    "        threshold: Threshold de decis√£o\n",
    "    \"\"\"\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=[\"No Default\", \"Default\"])\n",
    "\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(f\"Confusion Matrix (threshold={threshold:.3f})\")\n",
    "    plt.show()\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    print(\"üìä Impacto de neg√≥cio:\")\n",
    "    print(f\"TP (inadimplentes detectados): {tp}\")\n",
    "    print(f\"FN (inadimplentes aprovados ‚ùå): {fn}\")\n",
    "    print(f\"FP (bons clientes recusados): {fp}\")\n",
    "    print(f\"TN (bons clientes aprovados): {tn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Prepara√ß√£o dos Dados\n",
    "\n",
    "### Por que esse pr√©-processamento?\n",
    "- **MinMaxScaler**: Normaliza features num√©ricas para [0,1]\n",
    "- **OneHotEncoder**: Converte categ√≥ricas em vari√°veis dummy\n",
    "- **ColumnTransformer**: Aplica transforma√ß√µes espec√≠ficas por tipo\n",
    "\n",
    "### Por que Stratified Split?\n",
    "Mant√©m a **propor√ß√£o de classes** no treino e teste, evitando vi√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, params):\n",
    "    \"\"\"Pipeline completo de prepara√ß√£o dos dados.\n",
    "    \n",
    "    1. Feature Engineering\n",
    "    2. Sele√ß√£o de features\n",
    "    3. Defini√ß√£o de tipos\n",
    "    4. Pr√©-processamento\n",
    "    5. Train/Test Split\n",
    "    \"\"\"\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    # ===============================\n",
    "    # TARGET\n",
    "    # ===============================\n",
    "    y = df['default.payment.next.month']\n",
    "\n",
    "    # ===============================\n",
    "    # FEATURES SELECIONADAS\n",
    "    # ===============================\n",
    "    X = df[\n",
    "        [\n",
    "            'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE',\n",
    "            'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6',\n",
    "            'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3',\n",
    "            'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',\n",
    "            'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "            'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n",
    "            'PCT_PAID_MEAN',        # Feature engineered\n",
    "            'CREDIT_UTILIZATION',   # Feature engineered\n",
    "            'PAY_DELAY_MEAN',       # Feature engineered\n",
    "            'PAY_DELAY_MAX'         # Feature engineered\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    # ===============================\n",
    "    # DEFINI√á√ÉO DOS TIPOS\n",
    "    # ===============================\n",
    "    binary_features = ['SEX']\n",
    "    categorical_features = ['EDUCATION', 'MARRIAGE']\n",
    "    numerical_features = [col for col in X.columns if col not in binary_features + categorical_features]\n",
    "    \n",
    "    # ===============================\n",
    "    # PREPROCESSAMENTO\n",
    "    # ===============================\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('bin', MinMaxScaler(), binary_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "            ('num', MinMaxScaler(), numerical_features)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # ===============================\n",
    "    # STRATIFIED SPLIT\n",
    "    # ===============================\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=params[\"test_size\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "        stratify=y  # Mant√©m propor√ß√£o de classes\n",
    "    )\n",
    "    \n",
    "    # ===============================\n",
    "    # TRANSFORMAR DADOS\n",
    "    # ===============================\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Cross-Validation para Sele√ß√£o de Lambda\n",
    "\n",
    "### Por que Cross-Validation?\n",
    "Evita **overfitting** e garante que o modelo generalize bem.\n",
    "\n",
    "### Por que regulariza√ß√£o L2?\n",
    "- Previne overfitting\n",
    "- Melhora generaliza√ß√£o\n",
    "- Reduz vari√¢ncia do modelo\n",
    "\n",
    "### Por que ROC-AUC como m√©trica?\n",
    "- Invariante ao threshold\n",
    "- Mede capacidade de separa√ß√£o das classes\n",
    "- Padr√£o em problemas de classifica√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_lambda(X, y, lambdas, params):\n",
    "    \"\"\"Seleciona melhor lambda via Cross-Validation.\n",
    "    \n",
    "    Testa diferentes valores de regulariza√ß√£o L2 e escolhe\n",
    "    o que maximiza ROC-AUC na valida√ß√£o cruzada.\n",
    "    \n",
    "    Args:\n",
    "        X: Features de treino\n",
    "        y: Target de treino\n",
    "        lambdas: Lista de valores lambda para testar\n",
    "        params: Par√¢metros de configura√ß√£o\n",
    "    \n",
    "    Returns:\n",
    "        best_lambda: Melhor valor de regulariza√ß√£o\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=params[\"cv_folds\"],\n",
    "        shuffle=True,\n",
    "        random_state=params[\"random_state\"]\n",
    "    )\n",
    "\n",
    "    best_lambda = None\n",
    "    best_auc = -np.inf\n",
    "\n",
    "    for lambda_ in lambdas:\n",
    "        aucs = []\n",
    "\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "\n",
    "            X_tr, X_val = X[train_idx], X[val_idx]\n",
    "            y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            # Arquitetura da Rede Neural\n",
    "            model = Sequential([\n",
    "                Dense(64, activation='relu',\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "                Dense(32, activation='relu',\n",
    "                      kernel_regularizer=tf.keras.regularizers.l2(lambda_)),\n",
    "                Dense(1, activation='linear')  # Logits para BinaryCrossentropy\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer=tf.keras.optimizers.Adam(\n",
    "                    learning_rate=params[\"learning_rate\"]\n",
    "                ),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "            )\n",
    "\n",
    "            model.fit(\n",
    "                X_tr,\n",
    "                y_tr,\n",
    "                epochs=params[\"epochs_cross\"],\n",
    "                verbose=params[\"verbose\"]\n",
    "            )\n",
    "\n",
    "            # Predi√ß√£o e c√°lculo de AUC\n",
    "            logits = model(X_val)\n",
    "            y_pred_proba = tf.nn.sigmoid(logits).numpy().ravel()\n",
    "\n",
    "            auc = roc_auc_score(y_val, y_pred_proba)\n",
    "            aucs.append(auc)\n",
    "\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "\n",
    "        print(f\"[CV] Œª={lambda_:.5f} | AUC={mean_auc:.4f} ¬± {std_auc:.4f}\")\n",
    "\n",
    "        if mean_auc > best_auc:\n",
    "            best_auc = mean_auc\n",
    "            best_lambda = lambda_\n",
    "\n",
    "    return best_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Treinamento do Modelo Final\n",
    "\n",
    "### Arquitetura da Rede Neural\n",
    "- **Camada 1**: 64 neur√¥nios + ReLU + L2 regularization\n",
    "- **Camada 2**: 32 neur√¥nios + ReLU + L2 regularization  \n",
    "- **Output**: 1 neur√¥nio + linear (logits)\n",
    "\n",
    "### Por que essa arquitetura?\n",
    "- **64 ‚Üí 32**: Redu√ß√£o progressiva captura padr√µes complexos\n",
    "- **ReLU**: Ativa√ß√£o n√£o-linear eficiente\n",
    "- **Linear output**: Para usar BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(X_train, X_test, y_train, y_test, best_lambda, params):\n",
    "    \"\"\"Treina modelo final com melhor lambda encontrado.\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: Features de treino e teste\n",
    "        y_train, y_test: Targets de treino e teste\n",
    "        best_lambda: Melhor regulariza√ß√£o encontrada no CV\n",
    "        params: Par√¢metros de configura√ß√£o\n",
    "    \n",
    "    Returns:\n",
    "        y_proba: Probabilidades preditas no test set\n",
    "        auc: ROC-AUC no test set\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(best_lambda)),\n",
    "        Dense(32, activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(best_lambda)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=params[\"learning_rate\"]\n",
    "        ),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=params[\"epochs\"],\n",
    "        verbose=params[\"verbose\"]\n",
    "    )\n",
    "\n",
    "    # Predi√ß√£o final\n",
    "    logits = model(X_test)\n",
    "    y_proba = tf.nn.sigmoid(logits).numpy().ravel()\n",
    "\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"üìä Neural Network ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "    return y_proba, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Otimiza√ß√£o de Threshold\n",
    "\n",
    "### Por que otimizar threshold?\n",
    "O threshold padr√£o (0.5) **n√£o √© √≥timo** para problemas de neg√≥cio.\n",
    "\n",
    "### Por que F1-score?\n",
    "- Balanceia Precision e Recall\n",
    "- Adequado para classes desbalanceadas\n",
    "- Foca na classe minorit√°ria (inadimplentes)\n",
    "\n",
    "### Impacto no Neg√≥cio\n",
    "Threshold menor ‚Üí Mais conservador ‚Üí Menos FN (inadimplentes aprovados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(y_true, y_proba):\n",
    "    \"\"\"Encontra threshold √≥timo maximizando F1-score.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Labels verdadeiros\n",
    "        y_proba: Probabilidades preditas\n",
    "    \n",
    "    Returns:\n",
    "        best_threshold: Threshold que maximiza F1-score\n",
    "    \"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)\n",
    "\n",
    "    # Calcula F1-score para cada threshold\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_f1 = f1_scores[best_idx]\n",
    "\n",
    "    print(f\"üéØ Melhor threshold: {best_threshold:.3f}\")\n",
    "    print(f\"üìà Melhor F1-score: {best_f1:.4f}\")\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualiza√ß√£o da Curva ROC\n",
    "\n",
    "### Por que Curva ROC?\n",
    "- Mostra **trade-off** entre TPR e FPR\n",
    "- Independente do threshold\n",
    "- AUC resume performance em um n√∫mero\n",
    "- Permite comparar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curves(y_test, preds_dict):\n",
    "    \"\"\"Plota curvas ROC para compara√ß√£o de modelos.\n",
    "    \n",
    "    Args:\n",
    "        y_test: Labels verdadeiros\n",
    "        preds_dict: Dicion√°rio {nome_modelo: probabilidades}\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for name, y_proba in preds_dict.items():\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "\n",
    "    # Linha de baseline (classificador aleat√≥rio)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Execu√ß√£o do Pipeline Completo\n",
    "\n",
    "### Fluxo de Execu√ß√£o:\n",
    "1. **Prepara√ß√£o dos dados** com feature engineering\n",
    "2. **Cross-validation** para sele√ß√£o de lambda\n",
    "3. **Treinamento** do modelo final\n",
    "4. **Otimiza√ß√£o** do threshold\n",
    "5. **Avalia√ß√£o** com matriz de confus√£o e ROC\n",
    "\n",
    "### Resultados Esperados:\n",
    "- ROC-AUC ~ 0.77\n",
    "- Threshold √≥timo ~ 0.22\n",
    "- F1-score ~ 0.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] Œª=0.00000 | AUC=0.7669 ¬± 0.0072\n",
      "[CV] Œª=0.00001 | AUC=0.7683 ¬± 0.0057\n",
      "[CV] Œª=0.00010 | AUC=0.7658 ¬± 0.0040\n",
      "[CV] Œª=0.00050 | AUC=0.7576 ¬± 0.0069\n",
      "[CV] Œª=0.00100 | AUC=0.7521 ¬± 0.0056\n",
      "üèÜ Melhor lambda escolhido via CV: 1.0e-05\n",
      "üìä Neural Network ROC-AUC: 0.7729\n",
      "\n",
      "üîé Threshold √≥timo (Neural Network)\n",
      "üéØ Melhor threshold: 0.262\n",
      "üìà Melhor F1-score: 0.5387\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Pipeline principal do projeto.\"\"\"\n",
    "     \n",
    "    # 1. Prepara√ß√£o dos dados\n",
    "    X_train, X_test, y_train, y_test = prepare_data(df, conf[\"parameters\"])\n",
    "\n",
    "    # 2. Cross-validation para sele√ß√£o de lambda\n",
    "    best_lambda = cross_validate_lambda(X_train, y_train, lambdas, conf[\"parameters\"])\n",
    "\n",
    "    print(f\"üèÜ Melhor lambda escolhido via CV: {best_lambda:.1e}\")\n",
    "\n",
    "    # 3. Treinamento do modelo final\n",
    "    nn_proba, _ = train_final_model(\n",
    "        X_train, X_test, y_train, y_test, best_lambda, conf[\"parameters\"]\n",
    "    )\n",
    "\n",
    "    # 4. Otimiza√ß√£o de threshold\n",
    "    print(\"\\nüîé Threshold √≥timo (Neural Network)\")\n",
    "    best_threshold = find_best_threshold(y_test, nn_proba)\n",
    "\n",
    "    # 5. Avalia√ß√£o final\n",
    "    plot_confusion_matrix(y_test, nn_proba, best_threshold)\n",
    "\n",
    "    plot_roc_curves(\n",
    "        y_test,\n",
    "        {\"Neural Network\": nn_proba}\n",
    "    )\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä An√°lise dos Resultados\n",
    "\n",
    "### üéØ Performance do Modelo\n",
    "- **ROC-AUC**: 0.7729 (Bom poder de separa√ß√£o)\n",
    "- **Melhor Lambda**: 1e-05 (Regulariza√ß√£o leve)\n",
    "- **Threshold √ìtimo**: 0.262 (Mais conservador que 0.5)\n",
    "- **F1-Score**: 0.5387 (Balanceado para classes desbalanceadas)\n",
    "\n",
    "### üíº Impacto de Neg√≥cio\n",
    "A matriz de confus√£o mostra:\n",
    "- **TP**: Inadimplentes corretamente identificados\n",
    "- **FN**: Inadimplentes aprovados (RISCO ALTO) ‚ùå\n",
    "- **FP**: Bons clientes recusados (OPORTUNIDADE PERDIDA)\n",
    "- **TN**: Bons clientes aprovados ‚úÖ\n",
    "\n",
    "### üîç Insights\n",
    "1. **Feature Engineering** foi crucial para o desempenho\n",
    "2. **Regulariza√ß√£o L2** preveniu overfitting\n",
    "3. **Threshold otimizado** reduziu falsos negativos\n",
    "4. **Cross-validation** garantiu robustez do modelo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
